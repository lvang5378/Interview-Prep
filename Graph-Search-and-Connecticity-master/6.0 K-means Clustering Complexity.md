##Complexity:
https://en.wikipedia.org/wiki/K-means_clustering#Complexity

####Using MST, kraskal to do clustering, complesity can be : O(ElogE) 
    O(ElogE+ElogV) precisely, E can be V^2 at most

####K-means NP-Hard
    - NP-hard in general Euclidean space d even for 2 clusters[12][13]
    - NP-hard for a general number of clusters k even in the plane[14]
    - If k and d (the dimension) are fixed, the problem can be exactly solved in time O(n^{dk+1}\log{n}), 
        where n is the number of entities to be clustered[15]

####Lloyd's : O(nkdi) 
   where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence. On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyds algorithm is therefore often considered to be of **"linear"** complexity in practice.  
   Lloyd's k-means algorithm has polynomial smoothed running time. It is shown that[11] for arbitrary set of n points in [0,1]^d, if each point is independently perturbed by a normal distribution with mean 0 and variance \sigma^2, then the expected running time of k-means algorithm is bounded by O( n^{34}k^{34}d^8 \log^4(n)/ \sigma^6 ), which is a polynomial in n, k, d and 1/\sigma.  
   Lloyd's algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use the triangle inequality in order to create bounds and accelerate Lloyds algorithm
